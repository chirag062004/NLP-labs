{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BdUezBOnaIR"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Quora Text Classification Data.csv')\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "from tensorflow.keras.models import Sequential  # Sequential model for neural network\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Embedding  # Neural network layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Text tokenization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Sequence padding\n",
        "from nltk.tokenize import word_tokenize  # Text tokenization\n",
        "from nltk.stem import WordNetLemmatizer  # Word lemmatization\n",
        "from nltk.corpus import stopwords  # Stopwords removal\n",
        "from string import punctuation  # Punctuation characters\n",
        "import numpy as np  # Numerical operations\n",
        "from tqdm import tqdm  # Progress bar\n",
        "tqdm.pandas()  # Enable pandas progress_apply() with progress bar\n",
        "\n",
        "\"\"\"\n",
        "The dataset contains Quora questions with labels indicating whether they're sincere (0) or insincere (1)\n",
        "Columns: qid (question id), question_text, target (0/1)\n",
        "\"\"\"\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/Quora Text Classification Data.csv')\n",
        "df.head()  # Display first 5 rows to inspect data\n",
        "\n",
        "# Download NLTK resources (stopwords, tokenizer models, wordnet)\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Common words to be removed (the, is, etc.)\n",
        "nltk.download('punkt')  # Pre-trained tokenizer model\n",
        "nltk.download('wordnet')  # Lexical database for lemmatization\n",
        "\n",
        "# Combine standard English stopwords with punctuation marks\n",
        "stop_words = stopwords.words('english') + list(punctuation)\n",
        "lem = WordNetLemmatizer()  # Initialize lemmatizer (reduces words to base form)\n",
        "\n",
        "def cleaning(text):\n",
        "    \"\"\"\n",
        "    Text preprocessing function:\n",
        "    1. Convert to lowercase\n",
        "    2. Tokenize into words\n",
        "    3. Remove stopwords and punctuation\n",
        "    4. Lemmatize words\n",
        "    5. Rejoin into single string\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Normalize case\n",
        "    words = word_tokenize(text)  # Split text into tokens\n",
        "    words = [w for w in words if w not in stop_words]  # Filter stopwords\n",
        "    words = [lem.lemmatize(w) for w in words]  # Reduce words to base form\n",
        "    return ' '.join(words)  # Rejoin into single string\n",
        "\n",
        "# Apply cleaning function to all questions with progress bar\n",
        "df['Clean Text'] = df['question_text'].progress_apply(cleaning)\n",
        "\n",
        "# Unzip GloVe word embeddings (pre-trained word vectors)\n",
        "# GloVe: Global Vectors for Word Representation - captures semantic relationships\n",
        "!unzip '/content/drive/MyDrive/Word Embeddings/glove.42B.300d.zip'\n",
        "\n",
        "# Load GloVe embeddings into dictionary {word: vector}\n",
        "embedding_values = {}\n",
        "f = open('/content/glove.42B.300d.txt')  # 42B tokens, 300-dimensional vectors\n",
        "for line in tqdm(f):\n",
        "    value = line.split(' ')\n",
        "    word = value[0]  # First element is the word\n",
        "    coef = np.array(value[1:], dtype=\"float32\")  # Remaining elements are the vector\n",
        "    if coef is not None:\n",
        "        embedding_values[word] = coef\n",
        "\n",
        "# Initialize tokenizer - converts text to sequences of integers\n",
        "tokenizer = Tokenizer()\n",
        "x = df['Clean Text']  # Features (cleaned text)\n",
        "y = df['target']  # Labels (0 or 1)\n",
        "\n",
        "# Build vocabulary from all texts\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "seq = tokenizer.texts_to_sequences(x)\n",
        "# Pad sequences to ensure uniform length (300 tokens)\n",
        "pad_seq = pad_sequences(seq, maxlen=300)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for OOV (out-of-vocabulary) token\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Create embedding matrix where each row corresponds to a word in our vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, 300))  # Initialize with zeros\n",
        "for word, i in tqdm(tokenizer.word_index.items()):\n",
        "    value = embedding_values.get(word)\n",
        "    if value is not None:\n",
        "        embedding_matrix[i] = value  # Assign pre-trained vector if word exists in GloVe\n",
        "\n",
        "\"\"\"\n",
        "Model Architecture:\n",
        "1. Embedding Layer: Uses pre-trained GloVe vectors (frozen during training)\n",
        "2. LSTM Layer: Processes sequential information (50 units)\n",
        "3. Dense Layer: 128 ReLU units for feature transformation\n",
        "4. Output Layer: Single sigmoid unit for binary classification\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "# Embedding layer with pre-trained weights (trainable=False to keep fixed)\n",
        "model.add(Embedding(vocab_size, 300, input_length=300,\n",
        "                   weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(50, return_sequences=False))  # Single LSTM layer\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
        "\n",
        "# Compile model with Adam optimizer and binary crossentropy loss\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with 20% validation split\n",
        "history = model.fit(pad_seq, y, validation_split=0.2, epochs=5)"
      ]
    }
  ]
}